import json
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Optional, Literal
from google import genai
from google.genai import types
import os

# --- Execution Instructions ---
# To run this service, ensure you have all dependencies installed:
# pip install fastapi uvicorn pydantic google-genai
# Then, run the API using uvicorn:
# uvicorn app:app --reload
# ------------------------------

# --- Configuration ---
# NOTE: API_KEY should be loaded from environment variables in a real application.
API_KEY = os.environ.get("GEMINI_API_KEY", "") 
MODEL_NAME = "gemini-2.5-flash-preview-09-2025"

# Initialize the Gemini Client outside the endpoint for efficiency
try:
    gemini_client = genai.Client(api_key=API_KEY)
except Exception as e:
    # If API key is missing, this will likely fail; better to handle at runtime or via environment variables
    print(f"Warning: Could not initialize Gemini client. API calls may fail. {e}")

# Initialize FastAPI application
app = FastAPI()

# ----------------------------------------------------------------------
# --- 1. Pydantic Models for Request (Input) ---
# ----------------------------------------------------------------------

class TestResult(BaseModel):
    name: str
    status: str
    input: str
    expect: str
    actual: str

class AssignmentContext(BaseModel):
    content: str
    language: str = Field(..., description="E.g., C, C++, Python")

class Submission(BaseModel):
    code: str

class ReviewRequest(BaseModel):
    """The complete input payload for the code review endpoint."""
    assignment: AssignmentContext
    student_submission: Submission
    test_results: List[TestResult]


# ----------------------------------------------------------------------
# --- 2. Agentic Models (Tool Input/Output) ---
# ----------------------------------------------------------------------

class Testcase(BaseModel):
    name: str = Field(..., description="Short, descriptive name for the test case (e.g., 'Leading Spaces', 'Mixed Case Input').")
    input: str = Field(..., description="The exact input string to be passed to the student's function.")
    expect: str = Field(..., description="The correct, expected output string for this input based on the assignment.")

class GeneratedTests(BaseModel):
    """Structured output for the model to generate new tests."""
    new_tests: List[Testcase]

# Schema dictionary for generating new tests
GENERATED_TESTS_SCHEMA_DICT = {
    "type": "OBJECT",
    "properties": {
        "new_tests": {
            "type": "ARRAY",
            "items": {
                "type": "OBJECT",
                "properties": {
                    "name": {"type": "STRING"},
                    "input": {"type": "STRING"},
                    "expect": {"type": "STRING"}
                },
                "required": ["name", "input", "expect"]
            }
        }
    },
    "required": ["new_tests"]
}


# ----------------------------------------------------------------------
# --- 3. Review Response Models (Final Output) ---
# ----------------------------------------------------------------------

class LineContext(BaseModel):
    start: int
    end: int

class ColumnContext(BaseModel):
    start: int
    end: int

class ReviewItem(BaseModel):
    line: LineContext
    column: Optional[ColumnContext] = None 
    code_snippet: str
    type: Literal["Error", "Warning", "Correctness"]
    issue: str = Field(..., description="For a 'Warning', this must describe a case where it can lead to a bug.")
    fix_suggestion: str

class ReviewResponse(BaseModel):
    """The structured output generated by the Gemini Model."""
    summary: str
    detail: str
    review_items: List[ReviewItem]


# ----------------------------------------------------------------------
# --- 4. Gemini API Configuration and Schema (Constants) ---
# ----------------------------------------------------------------------

SYSTEM_INSTRUCTION = (
    "Act as a Computer Science 1 (CS1) course instructor reviewing a student's submission. "
    "Your review should focus on logical correctness, adherence to the assignment's "
    "specifications (especially the use of 'outstr'), basic C array manipulation principles, "
    "and code clarity. "
    "Crucially, the 'line' (start and end line numbers) and 'column' (start and end column indices within the line) in the JSON output MUST accurately pinpoint the 'code_snippet' being discussed. "
    "For all code sections, including those classified as 'Correctness', always suggest ways "
    "to improve code quality, clarity, efficiency, or adherence to best practices where possible. "
    "You MUST classify each review item into one of the following types: "
    "1. **Error**: A definitive bug that causes incorrect behavior or crash. "
    "2. **Warning**: A potential bug or poor practice. The 'issue' field for Warnings MUST clearly state a case where it can lead to a bug. "
    "3. **Correctness**: Something the student did well. The 'fix_suggestion' for these items MUST offer advice on how to make the code cleaner, more efficient, or more idiomatic, even if the current code is functional. "
    "Your task is to perform a detailed code review and you MUST generate "
    "the review STRICTLY in the JSON format defined by the responseSchema."
)

REVIEW_RESPONSE_SCHEMA_DICT = {
    "type": "OBJECT",
    "properties": {
        "summary": {"type": "STRING", "description": "A high-level summary of the code's flaws."},
        "detail": {"type": "STRING", "description": "More detailed context on the main areas needing improvement."},
        "review_items": {
            "type": "ARRAY",
            "items": {
                "type": "OBJECT",
                "properties": {
                    "line": {
                        "type": "OBJECT",
                        "properties": {"start": {"type": "INTEGER"}, "end": {"type": "INTEGER"}}
                    },
                    "column": {
                        "type": "OBJECT",
                        "properties": {"start": {"type": "INTEGER"}, "end": {"type": "INTEGER"}}
                    },
                    "code_snippet": {"type": "STRING"},
                    "type": {"type": "STRING", "enum": ["Error", "Warning", "Correctness"]},
                    "issue": {"type": "STRING", "description": "What is wrong with this snippet. For a 'Warning', describe the case where it can lead to a bug."},
                    "fix_suggestion": {"type": "STRING", "description": "Specific advice on how to fix the issue."}
                },
                "required": ["line", "code_snippet", "type", "issue", "fix_suggestion"]
            }
        }
    },
    "required": ["summary", "detail", "review_items"]
}


# ----------------------------------------------------------------------
# --- 5. Tool Implementation (MOCK EXECUTION & Agentic Functions) ---
# ----------------------------------------------------------------------

def execute_code_and_get_results(code: str, new_tests: List[Testcase]) -> List[TestResult]:
    """
    MOCK FUNCTION: In a real agentic application, this would call a secure code
    execution sandbox.
    """
    mock_results = []
    for test in new_tests:
        if test.input == "  start and end  ":
            status = "FAIL"
            actual = "start and end " 
        elif test.input == "multiple    spaces":
            status = "PASS"
            actual = test.expect
        else:
            status = "PASS"
            actual = test.expect
            
        mock_results.append(TestResult(
            name=f"AGENT_{test.name}",
            status=status,
            input=test.input,
            expect=test.expect,
            actual=actual
        ))
    
    return mock_results


async def generate_new_tests_tool(assignment_content: str, student_code: str, existing_tests: List[TestResult]) -> List[Testcase]:
    """Uses Gemini to generate 2-3 new, challenging test cases."""

    test_generation_prompt = f"""
    The student has submitted the following code:
    --- CODE ---
    {student_code}
    --- ASSIGNMENT ---
    {assignment_content}
    --- EXISTING TEST RESULTS ---
    {json.dumps([t.model_dump() for t in existing_tests], indent=2)}

    Based on the assignment requirements and common student coding errors (especially edge cases like empty input, leading/trailing spaces, or maximum constraints), generate 2 new, highly challenging test cases.
    Generate the output strictly in the JSON format defined by the schema.
    """
    
    config = types.GenerateContentConfig(
        response_mime_type="application/json",
        response_schema=GENERATED_TESTS_SCHEMA_DICT,
        temperature=0.7, 
        system_instruction="You are an expert test engineer. Your only job is to create challenging edge-case test inputs and their corresponding correct outputs based on the provided code and assignment."
    )

    try:
        response = gemini_client.models.generate_content(
            model=MODEL_NAME,
            contents=test_generation_prompt,
            config=config
        )
        tests_data = json.loads(response.text.strip())
        return [Testcase(**t) for t in tests_data.get("new_tests", [])]

    except (genai.errors.APIError, json.JSONDecodeError, Exception) as e:
        print(f"Test Generation Failed: {e}")
        return []

# MODIFIED AGENTIC FUNCTION to enforce summary generation based on items
async def self_correct_review(initial_review_json: str, user_prompt: str, system_instruction: str) -> str:
    """
    Gemini reviews its own generated structured output (ReviewResponse) for adherence
    to the system instruction and schema, then corrects it if necessary.
    Also synthesizes the final 'summary' based on the 'review_items'.
    """
    correction_prompt = f"""
    You have generated the following code review JSON:
    --- INITIAL REVIEW JSON ---
    {initial_review_json}
    
    The original user prompt was:
    --- ORIGINAL QUERY ---
    {user_prompt}
    
    The STRICT rules for the output are:
    --- SYSTEM INSTRUCTION ---
    {system_instruction}
    
    Critique the INITIAL REVIEW JSON against all rules in the SYSTEM INSTRUCTION. 
    Specifically check:
    1. Does the JSON fully conform to the response schema (no missing fields, correct data types)?
    2. Does every 'Warning' item's 'issue' field clearly state a case where the code can lead to a bug?
    3. Does every 'Correctness' item's 'fix_suggestion' offer advice on cleaner, more efficient, or more idiomatic code?
    
    --- CRITICAL NEW INSTRUCTION: SUMMARY GENERATION ---
    4. BASED SOLELY ON THE CONTENT OF THE 'review_items' LIST, synthesize a high-level, concise overall review. ASSIGN this synthesized text to the 'summary' field of the final JSON response. The summary must reflect the balance of Errors, Warnings, and Correctness items. This step is mandatory, and you MUST update the 'summary' field.
    
    If the INITIAL REVIEW JSON has any issues, or if the 'summary' needs to be synthesized/updated based on the 'review_items' (which it must be), generate the CORRECTED, final JSON response that strictly follows ALL rules and the schema. Only output the final JSON.
    """
    
    # We use a slightly higher temperature for the self-critique/correction step 
    # to encourage deeper reflection.
    config = types.GenerateContentConfig(
        response_mime_type="application/json",
        response_schema=REVIEW_RESPONSE_SCHEMA_DICT,
        temperature=0.6, 
    )

    try:
        response = gemini_client.models.generate_content(
            model=MODEL_NAME,
            contents=correction_prompt,
            config=config
        )
        return response.text.strip()
    except (genai.errors.APIError, json.JSONDecodeError, Exception) as e:
        print(f"Self-Correction step failed: {e}. Returning initial review.")
        # Fallback: if the correction fails, return the initial, uncorrected JSON
        return initial_review_json


# ----------------------------------------------------------------------
# --- 6. FastAPI Endpoints (Agentic and Simple) ---
# ----------------------------------------------------------------------

@app.post("/review_code_agentic", response_model=ReviewResponse)
async def review_code_agentic(request: ReviewRequest):
    """
    ADVANCED AGENTIC ENDPOINT: Orchestrates Review Generation and Self-Correction/Refinement.
    (Steps 1 and 2 (Test Generation/Execution) are TEMPORARILY SKIPPED as requested.)
    """
    
    # --- TEMPORARILY SKIPPED AGENTIC STEPS 1 & 2 (Test Generation and Execution) ---
    print("Agent Step 1 & 2: Test Generation and Execution are temporarily skipped.")
    
    # Use existing test results for the review
    all_test_results = request.test_results[:] 
    
    # Prepare input for the initial review
    structured_input_data = request.model_dump()
    structured_input_data["test_results"] = [t.model_dump() for t in all_test_results]
    review_user_query = f"""
    Review the following student submission, which is structured as a JSON payload containing the assignment details, the student's code, and ALL test results.

    Analyze the code focusing on correctness, adherence to the output specification, and efficiency, using the provided test results as critical evidence of failure.

    JSON Input Data:
    {json.dumps(structured_input_data, indent=2)}
    """

    # Step 3 (Formerly Step 3): Generate Initial Review (focus on items/detail)
    print("Agent Step 3: Generating initial review...")
    initial_config = types.GenerateContentConfig(
        response_mime_type="application/json",
        response_schema=REVIEW_RESPONSE_SCHEMA_DICT,
        temperature=0.4, 
        system_instruction=SYSTEM_INSTRUCTION
    )

    try:
        initial_response = gemini_client.models.generate_content(
            model=MODEL_NAME,
            contents=review_user_query,
            config=initial_config
        )
        initial_review_json = initial_response.text.strip()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Initial Review Generation Failed: {e}")

    # Step 4 (Formerly Step 4): Self-Correction, Reflection, and SUMMARY GENERATION
    print("Agent Step 4: Performing self-correction, reflection, and final summary generation...")
    final_review_json = await self_correct_review(
        initial_review_json,
        review_user_query,
        SYSTEM_INSTRUCTION
    )

    try:
        # Final validation and return
        return json.loads(final_review_json)
    except json.JSONDecodeError:
        # If even the corrected version is bad JSON, raise an error
        raise HTTPException(status_code=500, detail="Agent failed to produce valid, final JSON after self-correction.")


@app.post("/review_code", response_model=ReviewResponse)
async def review_code(request: ReviewRequest):
    """
    SIMPLE ENDPOINT: Performs a single-pass review using only provided tests 
    (no agentic steps).
    """
    
    # ... (code for the simple endpoint remains unchanged) ...

    structured_input_data = request.model_dump()

    user_query = f"""
    Review the following student submission, which is structured as a JSON payload containing the assignment details, the student's code, and test results.

    Analyze the code focusing on correctness, adherence to the output specification, and efficiency, using the provided test results as critical evidence of failure.

    JSON Input Data:
    {json.dumps(structured_input_data, indent=2)}
    """

    config = types.GenerateContentConfig(
        response_mime_type="application/json",
        response_schema=REVIEW_RESPONSE_SCHEMA_DICT,
        temperature=0.1, 
        system_instruction=SYSTEM_INSTRUCTION
    )

    try:
        response = gemini_client.models.generate_content(
            model=MODEL_NAME,
            contents=user_query,
            config=config
        )
        
        review_output = response.text.strip()
        return json.loads(review_output)

    except genai.errors.APIError as e:
        raise HTTPException(status_code=502, detail=f"Gemini API Error: {e}")
    except json.JSONDecodeError:
        raise HTTPException(status_code=500, detail="LLM failed to return valid JSON conforming to the schema.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An unexpected server error occurred: {e}")